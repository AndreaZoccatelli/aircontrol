{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndreaZoccatelli/aircontrol/blob/master/mediapipe_fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWNCsufLRUEV"
      },
      "source": [
        "# Mediapipe fine tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use this notebook if you want to make the MediaPipe model able to recognise more gestures. The dataset used here is [HaGRID](https://github.com/hukenovs/hagrid), which provides 18 different gestures. For the demo, six of them have been selected."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axjMaoNdvtmS"
      },
      "source": [
        "## Install Required libraries + Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6DBLRE-fqlO5"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install opendatasets\n",
        "!pip install mediapipe-model-maker\n",
        "!pip install opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c74UL9oI0VKU"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import opendatasets as od\n",
        "import os\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "import cv2 as cv\n",
        "import numpy as np\n",
        "\n",
        "assert tf.__version__.startswith(\"2\")\n",
        "from random import sample, seed\n",
        "\n",
        "from mediapipe_model_maker import gesture_recognizer\n",
        "import mediapipe as mp\n",
        "from mediapipe.tasks import python\n",
        "from mediapipe.tasks.python import vision\n",
        "import json\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPsPM5Livyfi"
      },
      "source": [
        "## Download Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0vO8w1ssvxo"
      },
      "outputs": [],
      "source": [
        "od.download(\n",
        "    \"https://www.kaggle.com/datasets/innominate817/hagrid-classification-512p-127k\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgADP9dtwdxG"
      },
      "source": [
        "# Dataset Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIjrZfO8Pz5C"
      },
      "source": [
        "The original dataset contains pictures of standing people; this could be a problem, given that the desired use case is hand sign recognition near the PC.\n",
        "\n",
        "To make the training set more relevant for the final objective, the images are zoomed using the bounding box detected with the Mediapipe hand landmarker. To make the model more robust to different light conditions, the images are also modified with the function alter light."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TS1NlQWGcZcE"
      },
      "outputs": [],
      "source": [
        "!wget -q https://storage.googleapis.com/mediapipe-models/hand_landmarker/hand_landmarker/float16/1/hand_landmarker.task\n",
        "\n",
        "base_options = python.BaseOptions(model_asset_path='hand_landmarker.task')\n",
        "options = vision.HandLandmarkerOptions(base_options=base_options,\n",
        "                                       num_hands=1)\n",
        "detector = vision.HandLandmarker.create_from_options(options)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwOMEksYd5PE"
      },
      "outputs": [],
      "source": [
        "def bounding_box_resize(imgs_path, model, zoom_factor, failed=0, resize=True):\n",
        "    image = mp.Image.create_from_file(imgs_path)\n",
        "    detection_result = model.detect(image)\n",
        "    x_list = []\n",
        "    y_list = []\n",
        "    try:\n",
        "        img = image.numpy_view()\n",
        "        for k in detection_result.hand_landmarks[0]:\n",
        "            x_list.append(k.x)\n",
        "            y_list.append(k.y)\n",
        "        min_x = min(x_list) * img.shape[1]\n",
        "        min_x = round(min_x - zoom_factor * min_x)\n",
        "        max_x = max(x_list) * img.shape[1]\n",
        "        max_x = round(max_x + zoom_factor * max_x)\n",
        "        min_y = min(y_list) * img.shape[0]\n",
        "        min_y = round(min_y - zoom_factor * min_y)\n",
        "        max_y = max(y_list) * img.shape[0]\n",
        "        max_y = round(max_y + zoom_factor * max_y)\n",
        "        cropped_image = img[min_y:max_y, min_x:max_x]\n",
        "        if resize:\n",
        "            cropped_image = cv.resize(cropped_image, img.shape[0:2])\n",
        "\n",
        "        return cropped_image, failed\n",
        "\n",
        "    except (IndexError, Exception):\n",
        "        failed += 1\n",
        "        return img, failed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPeH2kNBijPk"
      },
      "outputs": [],
      "source": [
        "def alter_light(img, alpha, beta):\n",
        "    img_add_b = np.clip(img * 1.5, 0, 255).astype(np.uint8)\n",
        "    img_sub_b = np.clip(img * 0.5, 0, 255).astype(np.uint8)\n",
        "    noise = np.random.normal(loc=0, scale=20, size=img.shape).astype(np.uint8)\n",
        "    img_sub_b = cv.addWeighted(img_sub_b, alpha, noise, beta, 0)\n",
        "    return img_add_b, img_sub_b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "id6oRybwGHuc"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(\n",
        "    dataset_path,\n",
        "    chosen_gestures,\n",
        "    imgs_path,\n",
        "    sample_size,\n",
        "    random_zoom_in,\n",
        "    v_augment,\n",
        "    model=None,\n",
        "):\n",
        "\n",
        "    np.random.seed(123)\n",
        "\n",
        "    seed(123)\n",
        "\n",
        "    none_subfolder = os.path.join(dataset_path, \"none\")\n",
        "\n",
        "    if not os.path.exists(none_subfolder):\n",
        "\n",
        "        os.makedirs(none_subfolder)\n",
        "\n",
        "\n",
        "    for gesture in tqdm(chosen_gestures):\n",
        "\n",
        "        gesture_subfolder = os.path.join(dataset_path, gesture)\n",
        "\n",
        "        if not os.path.exists(gesture_subfolder):\n",
        "\n",
        "            os.makedirs(gesture_subfolder)\n",
        "\n",
        "\n",
        "        gesture_imgs_path = os.path.join(imgs_path, gesture)\n",
        "\n",
        "        gesture_imgs = os.listdir(gesture_imgs_path)\n",
        "\n",
        "        if sample_size is not None:\n",
        "\n",
        "            gesture_imgs = sample(gesture_imgs, sample_size)\n",
        "\n",
        "\n",
        "        if random_zoom_in:\n",
        "\n",
        "            failed = 0\n",
        "\n",
        "            gesture_imgs = np.array(gesture_imgs)\n",
        "\n",
        "            to_zoom = sample(\n",
        "                range(0, len(gesture_imgs)), round(0.5 * len(gesture_imgs))\n",
        "            )\n",
        "\n",
        "            to_zoom = gesture_imgs[to_zoom]\n",
        "\n",
        "            regular = np.setdiff1d(range(0, len(gesture_imgs)), to_zoom)\n",
        "\n",
        "            regular = gesture_imgs[regular]\n",
        "\n",
        "\n",
        "        for i in regular:\n",
        "\n",
        "            if v_augment:\n",
        "\n",
        "                img = cv.imread(os.path.join(gesture_imgs_path, i))\n",
        "\n",
        "                beta = np.random.choice([0.1, 0.2, 0.3])\n",
        "\n",
        "                alpha = 1 - beta\n",
        "\n",
        "                img_add_b, img_sub_b = alter_light(img, alpha, beta)\n",
        "\n",
        "                cv.imwrite(os.path.join(gesture_subfolder, \"add_b_\" + i), img_add_b)\n",
        "\n",
        "                cv.imwrite(os.path.join(gesture_subfolder, \"sub_b_\" + i), img_sub_b)\n",
        "\n",
        "            shutil.copy(\n",
        "                os.path.join(gesture_imgs_path, i), os.path.join(gesture_subfolder, i)\n",
        "            )\n",
        "\n",
        "\n",
        "        for i in to_zoom:\n",
        "\n",
        "            zoom_factor = np.random.choice([0.2, 0.3, 0.4])\n",
        "\n",
        "            cropped_image, failed = bounding_box_resize(\n",
        "                os.path.join(gesture_imgs_path, i),\n",
        "                model,\n",
        "                zoom_factor,\n",
        "                failed=failed,\n",
        "                resize=True,\n",
        "            )\n",
        "\n",
        "            if cropped_image is not None:\n",
        "\n",
        "                if v_augment:\n",
        "\n",
        "                    beta = np.random.choice([0.1, 0.2, 0.3])\n",
        "\n",
        "                    alpha = 1 - beta\n",
        "\n",
        "                    img_add_b, img_sub_b = alter_light(cropped_image, alpha, beta)\n",
        "\n",
        "                    cv.imwrite(\n",
        "                        os.path.join(gesture_subfolder, \"add_b_cropped_\" + i), img_add_b\n",
        "                    )\n",
        "\n",
        "                    cv.imwrite(\n",
        "                        os.path.join(gesture_subfolder, \"sub_b_cropped_\" + i), img_sub_b\n",
        "                    )\n",
        "\n",
        "                cv.imwrite(\n",
        "                    os.path.join(gesture_subfolder, \"cropped_\" + i), cropped_image\n",
        "                )\n",
        "\n",
        "        if random_zoom_in:\n",
        "\n",
        "            print(f\"{round(failed/len(to_zoom))}% no bounding box detected\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VlcPInWQS4bL"
      },
      "outputs": [],
      "source": [
        "shutil.rmtree(\"/content/Dataset\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXGPoEU1HiHz"
      },
      "outputs": [],
      "source": [
        "# write here the gestures on which you want to train the model\n",
        "chosen_gestures = [\"one\", \"peace\", \"three2\", \"four\", \"palm\", \"fist\"]\n",
        "imgs_path = \"/content/hagrid-classification-512p-127k/hagrid-classification-512p-127k\"\n",
        "dataset_path = os.path.join(\"/content\", \"Dataset\")\n",
        "if not os.path.exists(dataset_path):\n",
        "    os.makedirs(dataset_path)\n",
        "\n",
        "prepare_dataset(\n",
        "    dataset_path,\n",
        "    chosen_gestures,\n",
        "    imgs_path,\n",
        "    sample_size=None,\n",
        "    random_zoom_in=True,\n",
        "    model=detector,\n",
        "    v_augment=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T21yWliaoerc"
      },
      "outputs": [],
      "source": [
        "dataset_path = \"/content/Dataset\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FfdmgAGIJi13"
      },
      "outputs": [],
      "source": [
        "seed(123)\n",
        "none_subfolder = os.path.join(dataset_path, \"none\")\n",
        "others = os.listdir(imgs_path)\n",
        "others = np.setdiff1d(others, chosen_gestures)\n",
        "\n",
        "for folder_name in tqdm(others):\n",
        "    folder = os.path.join(imgs_path, folder_name)\n",
        "    files = os.listdir(folder)\n",
        "    files = sample(files, 200)\n",
        "\n",
        "    # Iterate over each file and copy it to the destination folder\n",
        "    for img in files:\n",
        "        shutil.copy(os.path.join(folder, img), none_subfolder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTTNZsolKXiT"
      },
      "outputs": [],
      "source": [
        "data = gesture_recognizer.Dataset.from_folder(\n",
        "    dirname=dataset_path, hparams=gesture_recognizer.HandDataPreprocessingParams()\n",
        ")\n",
        "train_data, rest_data = data.split(0.8)\n",
        "validation_data, test_data = rest_data.split(0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1tiLGGRcvhy"
      },
      "source": [
        "## Model training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QknWUEceEY1g"
      },
      "outputs": [],
      "source": [
        "shutil.rmtree(\"/content/exported_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CxMOI8o6iNLu"
      },
      "outputs": [],
      "source": [
        "hparams = gesture_recognizer.HParams(\n",
        "    learning_rate=0.001, export_dir=\"exported_model\", shuffle=True, batch_size=200\n",
        ")\n",
        "\n",
        "model_options = gesture_recognizer.ModelOptions(\n",
        "    dropout_rate=0.1, layer_widths=[256, 128, 64]\n",
        ")\n",
        "\n",
        "options = gesture_recognizer.GestureRecognizerOptions(\n",
        "    model_options=model_options, hparams=hparams\n",
        ")\n",
        "\n",
        "model = gesture_recognizer.GestureRecognizer.create(\n",
        "\n",
        "    train_data=train_data, validation_data=validation_data, options=options\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRH96bm-BbAo"
      },
      "outputs": [],
      "source": [
        "loss, accuracy = model.evaluate(test_data)\n",
        "print(f\"Test loss:{loss}, Test accuracy:{accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Xxjz8ZrO9UR"
      },
      "outputs": [],
      "source": [
        "model.export_model()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
